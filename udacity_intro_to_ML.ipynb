{
 "metadata": {
  "name": "",
  "signature": "sha256:e60281cb3d15f76921c19b9887b7775bfd7f2485b4bc60bcc419a6885de902b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn naive bayes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "Y = np.array([1, 1, 1, 2, 2, 2])\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "clf = GaussianNB()\n",
      "clf.fit(X, Y)\n",
      "print(clf.predict([[-0.8, -1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# prep_terrain_data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#!/usr/bin/python\n",
      "import random\n",
      "\n",
      "\n",
      "def makeTerrainData(n_points=1000):\n",
      "###############################################################################\n",
      "### make the toy dataset\n",
      "    random.seed(42)\n",
      "    grade = [random.random() for ii in range(0,n_points)]\n",
      "    bumpy = [random.random() for ii in range(0,n_points)]\n",
      "    error = [random.random() for ii in range(0,n_points)]\n",
      "    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]\n",
      "    for ii in range(0, len(y)):\n",
      "        if grade[ii]>0.8 or bumpy[ii]>0.8:\n",
      "            y[ii] = 1.0\n",
      "\n",
      "### split into train/test sets\n",
      "    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]\n",
      "    # zip: pair two arrays\n",
      "    split = int(0.75*n_points)\n",
      "    X_train = X[0:split]\n",
      "    X_test  = X[split:]\n",
      "    y_train = y[0:split]\n",
      "    y_test  = y[split:]\n",
      "\n",
      "    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
      "    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
      "    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
      "    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
      "\n",
      "#    training_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
      "#            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
      "\n",
      "\n",
      "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
      "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
      "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
      "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
      "\n",
      "    test_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
      "            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
      "\n",
      "    return X_train, y_train, X_test, y_test\n",
      "#    return training_data, test_data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from prep_terrain_data import makeTerrainData\n",
      "from class_vis import prettyPicture, output_image\n",
      "from ClassifyNB import classify\n",
      "\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "\n",
      "\n",
      "features_train, labels_train, features_test, labels_test = makeTerrainData ()\n",
      "\n",
      "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
      "### in together--separate them so we can give them different colors in the scatterplot,\n",
      "### and visually identify them\n",
      "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
      "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
      "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
      "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
      "\n",
      "\n",
      "\n",
      "clf = classify(features_train, labels_train)\n",
      "\n",
      "\n",
      "\n",
      "    ### draw the decision boundary with the text points overlaid\n",
      "prettyPicture(clf, features_test, labels_test)\n",
      "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# draw decision boundary"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#!/usr/bin/python\n",
      "\n",
      "#from udacityplots import *\n",
      "import matplotlib \n",
      "matplotlib.use('agg')\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pylab as pl\n",
      "import numpy as np\n",
      "\n",
      "#import numpy as np\n",
      "#import matplotlib.pyplot as plt\n",
      "#plt.ioff()\n",
      "\n",
      "def prettyPicture(clf, X_test, y_test):\n",
      "    x_min = 0.0; x_max = 1.0\n",
      "    y_min = 0.0; y_max = 1.0\n",
      "\n",
      "    # Plot the decision boundary. For that, we will assign a color to each\n",
      "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    h = .01  # step size in the mesh\n",
      "    xx, yy = np.meshgrid (np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "\n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    plt.xlim(xx.min(), xx.max())\n",
      "    plt.ylim(yy.min(), yy.max())\n",
      "\n",
      "    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)\n",
      "\n",
      "    # Plot also the test points\n",
      "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
      "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
      "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
      "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
      "\n",
      "    plt.scatter(grade_sig, bumpy_sig, color = \"b\", label=\"fast\")\n",
      "    plt.scatter(grade_bkg, bumpy_bkg, color = \"r\", label=\"slow\")\n",
      "    plt.legend()\n",
      "    plt.xlabel(\"bumpiness\")\n",
      "    plt.ylabel(\"grade\")\n",
      "\n",
      "    plt.savefig(\"test.png\")\n",
      "    \n",
      "import base64\n",
      "import json\n",
      "import subprocess\n",
      "\n",
      "def output_image(name, format, bytes):\n",
      "    image_start = \"BEGIN_IMAGE_f9825uweof8jw9fj4r8\"\n",
      "    image_end = \"END_IMAGE_0238jfw08fjsiufhw8frs\"\n",
      "    data = {}\n",
      "    data['name'] = name\n",
      "    data['format'] = format\n",
      "    data['bytes'] = base64.encodestring(bytes)\n",
      "    print image_start+json.dumps(data)+image_end"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# classifier "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "def classify (features_train, labels_train):   \n",
      "    ### import the sklearn module for GaussianNB\n",
      "    ### create classifier\n",
      "    ### fit the classifier on the training features and labels\n",
      "    ### return the fit classifier\n",
      "    \n",
      "        \n",
      "    ### your code goes here!\n",
      "    nb_classifier = GaussianNB ()\n",
      "    \n",
      "    return nb_classifier.fit (features_train, labels_train)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn: calculate accuracy"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def NBAccuracy(features_train, labels_train, features_test, labels_test):\n",
      "    \"\"\" compute the accuracy of your Naive Bayes classifier \"\"\"\n",
      "    ### import the sklearn module for GaussianNB\n",
      "    from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "    ### create classifier\n",
      "    clf = GaussianNB()\n",
      "    #TODO\n",
      "    clf.fit(features_train, labels_train)\n",
      "\n",
      "    ### fit the classifier on the training features and labels\n",
      "    #TODO\n",
      "\n",
      "    ### use the trained classifier to predict labels for the test features\n",
      "    pred = clf.predict(features_test)\n",
      "    #TODO\n",
      "\n",
      "\n",
      "    ### calculate and return the accuracy on the test data\n",
      "    ### this is slightly different than the example, \n",
      "    ### where we just print the accuracy\n",
      "    ### you might need to import an sklearn module\n",
      "    from sklearn.metrics import accuracy_score\n",
      "    accuracy = accuracy_score (labels_test, pred)\n",
      "    #TODO\n",
      "    return accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# calculate running time"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "t0 = time()\n",
      "< your clf.fit() line of code >\n",
      "print \"training time:\", round(time()-t0, 3), \"s\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SVM"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "import sys\n",
      "from class_vis import prettyPicture\n",
      "from prep_terrain_data import makeTerrainData\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import copy\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "\n",
      "\n",
      "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
      "\n",
      "\n",
      "########################## SVM #################################\n",
      "### we handle the import statement and SVC creation for you here\n",
      "from sklearn.svm import SVC\n",
      "clf = SVC(kernel=\"linear\")\n",
      "\n",
      "clf.fit (features_train, labels_train)\n",
      "pred = clf.predict (features_test)\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "acc = accuracy_score(pred, labels_test)\n",
      "\n",
      "def submitAccuracy():\n",
      "    return acc"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn: Decision Tree"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def classify(features_train, labels_train):\n",
      "    \n",
      "    ### your code goes here--should return a trained decision tree classifer\n",
      "\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "    clf = DecisionTreeClassifier ()\n",
      "    clf.fit (features_train, labels_train)\n",
      "    \n",
      "    return clf"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#!/usr/bin/python\n",
      "\n",
      "\"\"\" lecture and example code for decision tree unit \"\"\"\n",
      "\n",
      "import sys\n",
      "from class_vis import prettyPicture, output_image\n",
      "from prep_terrain_data import makeTerrainData\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "from classifyDT import classify\n",
      "\n",
      "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
      "\n",
      "### the classify() function in classifyDT is where the magic\n",
      "### happens--it's your job to fill this in!\n",
      "clf = classify(features_train, labels_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### grader code, do not modify below this line\n",
      "\n",
      "prettyPicture(clf, features_test, labels_test)\n",
      "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn Linear Regression"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "import numpy\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from ages_net_worths import ageNetWorthData\n",
      "\n",
      "ages_train, ages_test, net_worths_train, net_worths_test = ageNetWorthData()\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "reg = LinearRegression()\n",
      "reg.fit(ages_train, net_worths_train)\n",
      "\n",
      "### get Katie's net worth (she's 27)\n",
      "### sklearn predictions are returned in an array,\n",
      "### so you'll want to do something like net_worth = predict([27])[0]\n",
      "### (not exact syntax, the point is that [0] at the end)\n",
      "km_net_worth = reg.predict ([27])[0] ### fill in the line of code to get the right value\n",
      "\n",
      "### get the slope\n",
      "### again, you'll get a 2-D array, so stick the [0][0] at the end\n",
      "slope = reg.coef_[0][0] ### fill in the line of code to get the right value\n",
      "\n",
      "### get the intercept\n",
      "### here you get a 1-D array, so stick [0] on the end to access\n",
      "### the info we want\n",
      "intercept = reg.intercept_[0] ### fill in the line of code to get the right value\n",
      "\n",
      "\n",
      "### get the score on test data\n",
      "test_score = reg.score (ages_train, net_worths_train) ### fill in the line of code to get the right value\n",
      "\n",
      "\n",
      "### get the score on the training data\n",
      "training_score = reg.score (ages_test, net_worths_test) ### fill in the line of code to get the right value\n",
      "\n",
      "\n",
      "\n",
      "def submitFit():\n",
      "    return {\"networth\":km_net_worth,\n",
      "            \"slope\":slope,\n",
      "            \"intercept\":intercept,\n",
      "            \"stats on test\":test_score,\n",
      "            \"stats on training\": training_score}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn preprocessing"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "import numpy\n",
      "\n",
      "weights = numpy.array ([4, 5, 6]) * 1.0\n",
      "\n",
      "min_max_scale = MinMaxScaler ()\n",
      "weights = min_max_scale.fit_transform (weights)\n",
      "\n",
      "print weights"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn count vectorizer and stemmer"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from sklearn.feature_extraction import CountVectorizer\n",
      "\n",
      "vct = vectorizer ()\n",
      "string_1 = \"\"\n",
      "string_2 = \"\"\n",
      "string = [string_1, string_2]\n",
      "bow = vectorizer.fit (string)\n",
      "bow = vectorizer.transform (string)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# get stop words\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# get root: stemmer\n",
      "\n",
      "a = stopwords.words (\"english\")\n",
      "\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "\n",
      "s = SnowballStemmer ('english')\n",
      "\n",
      "print s.stem ('beach')"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn tfidf"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from nltk.corpus import stopwords\n",
      "sw = stopwords.words(\"english\")\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer (stop_words = \"english\",lowercase = True)\n",
      "vectorizer.fit_transform (word_data)\n",
      "\n",
      "print len(vectorizer.get_feature_names())"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
      "                             stop_words='english')\n",
      "features_train = vectorizer.fit_transform(features_train)\n",
      "features_test  = vectorizer.transform(features_test).toarray()"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# sklearn cross validation & K-fold"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from sklearn.cross_validation import train_test_split, KFold\n",
      "\n",
      "features_train, features_test, labels_train, labels_test = train_test_split (word_data, authors, test_size=0.1, random_state=42)\n",
      "\n",
      "kf = KFold (# of items in the total dataset, # of folds)\n",
      "\n",
      "for train_indices, test_indices in kf:\n",
      "    f_train = [f_train [i] for i in train_indices]\n",
      "    f_test = [f_test [i] for i in test_indices]\n",
      "    l_train = [l_train [i] for i in train_indices]\n",
      "    l_test = [l_test [i] for i in test_indices]"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "GridCV is a way of systematically working through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The beauty is that it can work through many combinations in only a couple extra lines of code.\n",
      "\n",
      "Here's an example from the sklearn documentation, which can be found here:\n",
      "\n",
      "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "svr = svm.SVC()\n",
      "clf = grid_search.GridSearchCV(svr, parameters)\n",
      "clf.fit(iris.data, iris.target)\n",
      "\n",
      "Let's break this down line by line.\n",
      "\n",
      "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} \n",
      "A dictionary of the parameters, and the possible values you want to try for them. In this case, they're playing around with the kernel (possible choices are 'linear' and 'rbf'), and C (possible choices are 1 and 10).\n",
      "\n",
      "Then all the following (kernel, C) combinations are automatically generated: [('rbf', 1), ('rbf', 10), ('linear', 1), ('linear', 10)]. Each is used to train an SVM, and the performance is then assessed using cross-validation.\n",
      "\n",
      "svr = svm.SVC() \n",
      "This looks kind of like creating a classifier, just like we've been doing since the first lesson. But note that the \"clf\" isn't made until the next line--this is just saying what kind of algorithm to use. Another way to think about this is that the \"classifier\" isn't just the algorithm in this case, it's algorithm plus parameter values. Note that there's no monkeying around with the kernel or C; all that is handled in the next line.\n",
      "\n",
      "clf = grid_search.GridSearchCV(svr, parameters)\n",
      "This is where the first bit of magic happens; the classifier is being created. We pass the algorithm (svr) and the dictionary of parameters to try (parameters) and it generates a grid of parameter combinations to try.\n",
      "\n",
      "clf.fit(iris.data, iris.target)\n",
      "And the second bit of magic. The fit function now tries all the parameter combinations, and returns a fitted classifier that's automatically tuned to the optimal parameter combination. You can now access the parameter values via clf.best_estimator_."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}